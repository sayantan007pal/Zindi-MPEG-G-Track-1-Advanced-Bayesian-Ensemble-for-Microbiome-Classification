#!/usr/bin/env python3
"""
Test Enhanced Features for MPEG-G Microbiome Challenge
======================================================

This script tests the enhanced features generated by the advanced feature engineering
pipeline to evaluate improvements over the baseline 30% accuracy.

Author: Sayantan Pal
Date: 2025-09-20
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import json
import os

warnings.filterwarnings('ignore')

def test_enhanced_features():
    """Test enhanced features against baseline performance."""
    
    print("Testing Enhanced Features for MPEG-G Microbiome Challenge")
    print("=" * 60)
    
    # Load enhanced features and metadata
    base_dir = "/Users/sayantanpal100/Desktop/MPEG-G_ Decoding the Dialogue"
    features_path = f"{base_dir}/enhanced_features/enhanced_features_final.csv"
    metadata_path = f"{base_dir}/enhanced_features/enhanced_metadata_final.csv"
    
    print(f"Loading enhanced features from: {features_path}")
    print(f"Loading metadata from: {metadata_path}")
    
    # Load data
    features = pd.read_csv(features_path, index_col=0)
    metadata = pd.read_csv(metadata_path, index_col=0)
    
    print(f"\nDataset Info:")
    print(f"- Samples: {features.shape[0]}")
    print(f"- Features: {features.shape[1]}")
    print(f"- Classes: {metadata['symptom'].value_counts().to_dict()}")
    
    # Prepare target variable
    le = LabelEncoder()
    y = le.fit_transform(metadata['symptom'])
    class_names = le.classes_
    
    print(f"- Encoded classes: {dict(zip(range(len(class_names)), class_names))}")
    
    # Test multiple models
    models = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=3),
        'Random Forest (deeper)': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5),
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, C=0.1),
        'Logistic Regression (L1)': LogisticRegression(random_state=42, max_iter=1000, C=0.1, penalty='l1', solver='liblinear')
    }
    
    results = {}
    
    # Cross-validation setup
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    print(f"\nTesting Models with 5-Fold Cross-Validation:")
    print("-" * 50)
    
    for model_name, model in models.items():
        print(f"\nTesting {model_name}...")
        
        # Scale features for logistic regression
        if 'Logistic' in model_name:
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(features)
            cv_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='accuracy')
        else:
            cv_scores = cross_val_score(model, features, y, cv=cv, scoring='accuracy')
        
        mean_score = cv_scores.mean()
        std_score = cv_scores.std()
        
        results[model_name] = {
            'cv_mean': mean_score,
            'cv_std': std_score,
            'cv_scores': cv_scores.tolist()
        }
        
        print(f"  Cross-validation accuracy: {mean_score:.3f} ± {std_score:.3f}")
        print(f"  Individual fold scores: {[f'{score:.3f}' for score in cv_scores]}")
    
    # Find best model
    best_model_name = max(results.keys(), key=lambda k: results[k]['cv_mean'])
    best_score = results[best_model_name]['cv_mean']
    
    print(f"\nBest Model: {best_model_name}")
    print(f"Best CV Accuracy: {best_score:.3f} ± {results[best_model_name]['cv_std']:.3f}")
    
    # Compare with baseline
    baseline_accuracy = 0.30  # From previous results
    improvement = best_score - baseline_accuracy
    improvement_pct = (improvement / baseline_accuracy) * 100
    
    print(f"\nComparison with Baseline:")
    print(f"- Baseline accuracy: {baseline_accuracy:.3f}")
    print(f"- Enhanced accuracy: {best_score:.3f}")
    print(f"- Improvement: {improvement:+.3f} ({improvement_pct:+.1f}%)")
    
    # Train best model for detailed analysis
    best_model = models[best_model_name]
    
    if 'Logistic' in best_model_name:
        scaler = StandardScaler()
        X_for_training = scaler.fit_transform(features)
    else:
        X_for_training = features
    
    best_model.fit(X_for_training, y)
    
    # Feature importance analysis
    if hasattr(best_model, 'feature_importances_'):
        feature_importance = pd.Series(best_model.feature_importances_, index=features.columns)
        top_features = feature_importance.nlargest(15)
        
        print(f"\nTop 15 Most Important Features ({best_model_name}):")
        print("-" * 50)
        for i, (feature, importance) in enumerate(top_features.items(), 1):
            print(f"{i:2d}. {feature[:50]:<50} {importance:.4f}")
    
    # Create visualizations
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. Model comparison
    model_names = list(results.keys())
    cv_means = [results[name]['cv_mean'] for name in model_names]
    cv_stds = [results[name]['cv_std'] for name in model_names]
    
    bars = axes[0,0].bar(range(len(model_names)), cv_means, yerr=cv_stds, capsize=5)
    axes[0,0].axhline(y=baseline_accuracy, color='red', linestyle='--', label=f'Baseline ({baseline_accuracy:.1%})')
    axes[0,0].set_xticks(range(len(model_names)))
    axes[0,0].set_xticklabels([name.replace(' ', '\n') for name in model_names], rotation=0)
    axes[0,0].set_ylabel('Cross-Validation Accuracy')
    axes[0,0].set_title('Model Performance Comparison')
    axes[0,0].legend()
    axes[0,0].grid(True, alpha=0.3)
    
    # Add value labels on bars
    for bar, mean in zip(bars, cv_means):
        axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                      f'{mean:.3f}', ha='center', va='bottom')
    
    # 2. Feature importance (if available)
    if hasattr(best_model, 'feature_importances_'):
        top_features_plot = feature_importance.nlargest(10)
        feature_names_short = [name.replace('temporal_var_', '').replace('change_', '').replace('log_ratio_', '')[:20] 
                              for name in top_features_plot.index]
        
        axes[0,1].barh(range(len(top_features_plot)), top_features_plot.values)
        axes[0,1].set_yticks(range(len(top_features_plot)))
        axes[0,1].set_yticklabels(feature_names_short)
        axes[0,1].set_xlabel('Feature Importance')
        axes[0,1].set_title(f'Top 10 Features ({best_model_name})')
    
    # 3. Class distribution
    class_counts = metadata['symptom'].value_counts()
    colors = ['lightcoral', 'gold', 'lightblue', 'lightgreen'][:len(class_counts)]
    axes[1,0].pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%', colors=colors)
    axes[1,0].set_title('Class Distribution')
    
    # 4. CV scores distribution
    all_scores = []
    all_models = []
    for name, result in results.items():
        all_scores.extend(result['cv_scores'])
        all_models.extend([name.split()[0]] * len(result['cv_scores']))  # Shortened names
    
    score_df = pd.DataFrame({'Model': all_models, 'CV_Score': all_scores})
    sns.boxplot(data=score_df, x='Model', y='CV_Score', ax=axes[1,1])
    axes[1,1].axhline(y=baseline_accuracy, color='red', linestyle='--', alpha=0.7)
    axes[1,1].set_title('Cross-Validation Score Distribution')
    axes[1,1].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig(f'{base_dir}/enhanced_features/enhanced_model_performance.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # Save results
    results_summary = {
        'timestamp': pd.Timestamp.now().isoformat(),
        'dataset_info': {
            'n_samples': features.shape[0],
            'n_features': features.shape[1],
            'classes': class_names.tolist(),
            'class_distribution': metadata['symptom'].value_counts().to_dict()
        },
        'model_results': results,
        'best_model': best_model_name,
        'best_accuracy': best_score,
        'baseline_comparison': {
            'baseline_accuracy': baseline_accuracy,
            'enhanced_accuracy': best_score,
            'improvement': improvement,
            'improvement_percentage': improvement_pct
        },
        'feature_analysis': {
            'top_features': top_features.to_dict() if hasattr(best_model, 'feature_importances_') else None
        }
    }
    
    with open(f'{base_dir}/enhanced_features/enhanced_model_results.json', 'w') as f:
        json.dump(results_summary, f, indent=2)
    
    print(f"\nResults saved to: {base_dir}/enhanced_features/")
    print("- enhanced_model_performance.png: Performance visualization")
    print("- enhanced_model_results.json: Detailed results")
    
    # Final recommendations
    print(f"\nFinal Analysis:")
    print("=" * 50)
    
    if improvement > 0:
        print(f"✓ SUCCESS: Enhanced features improved accuracy by {improvement:.3f} ({improvement_pct:+.1f}%)")
        print(f"✓ New accuracy: {best_score:.3f} vs baseline {baseline_accuracy:.3f}")
        
        if improvement_pct > 50:
            print("✓ SIGNIFICANT IMPROVEMENT: Features show strong biological signal")
        elif improvement_pct > 20:
            print("✓ MODERATE IMPROVEMENT: Features provide meaningful enhancement")
        else:
            print("✓ MODEST IMPROVEMENT: Some benefit from feature engineering")
            
    else:
        print(f"⚠ WARNING: No improvement observed ({improvement:.3f})")
        print("  Consider: Different models, feature selection, or data preprocessing")
    
    print(f"\nKey Success Factors:")
    print(f"- Temporal features capture disease progression")
    print(f"- Dimensionality reduction helps with small sample size")
    print(f"- Feature engineering creates biologically meaningful variables")
    print(f"- Cross-validation provides robust performance estimates")
    
    return results_summary

if __name__ == "__main__":
    results = test_enhanced_features()